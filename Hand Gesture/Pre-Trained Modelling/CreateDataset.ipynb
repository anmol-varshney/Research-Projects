{"cells":[{"cell_type":"markdown","metadata":{"id":"7a713j1Vn0A4"},"source":["# Import Libraries\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbostzl5n0BN"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import os\n","from matplotlib import pyplot as plt\n","import time\n","import mediapipe as mp"]},{"cell_type":"markdown","metadata":{"id":"Thqcoqb7n0BV"},"source":["# KeyPoint detection using MP Holistic\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rbawrfin0BX"},"outputs":[],"source":["mp_holistic = mp.solutions.holistic # Holistic model\n","mp_drawing = mp.solutions.drawing_utils # Drawing utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulHSc0ibn0Ba"},"outputs":[],"source":["def mediapipe_detection(image, model):\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n","    image.flags.writeable = False                  # Image is no longer writeable\n","    results = model.process(image)                 # Make prediction\n","    image.flags.writeable = True                   # Image is now writeable \n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n","    return image, results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfmdQlYBn0Bd"},"outputs":[],"source":["def draw_landmarks(image, results):\n","    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n","    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n","    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n","    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQZB8o1Fn0Bg"},"outputs":[],"source":["def draw_styled_landmarks(image, results):\n","    # Draw face connections\n","    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n","                             mp_drawing.DrawingSpec(color = (255, 255, 255), #color=(80,110,10),\n","                              thickness=1, circle_radius=1), \n","                             mp_drawing.DrawingSpec(color= (255, 255, 255), #color=(80,256,121),\n","                              thickness=1, circle_radius=1)\n","                             ) \n","    # Draw pose connections\n","    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n","                             mp_drawing.DrawingSpec(color = (255, 255, 255), #color=(80,22,10),\n","                              thickness=2, circle_radius=4), \n","                             mp_drawing.DrawingSpec(color = (255, 255, 255), #color=(80,44,121),\n","                              thickness=2, circle_radius=2)\n","                             ) \n","    # Draw left hand connections\n","    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n","                             mp_drawing.DrawingSpec(color=(255, 255, 255), #color=(121,22,76),\n","                              thickness=2, circle_radius=4), \n","                             mp_drawing.DrawingSpec(color = (255, 255, 255), #color=(121,44,250),\n","                              thickness=2, circle_radius=2)\n","                             ) \n","    # Draw right hand connections  \n","    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n","                             mp_drawing.DrawingSpec(color=(255, 255, 255), #color=(245,117,66),\n","                              thickness=2, circle_radius=4), \n","                             mp_drawing.DrawingSpec(color=(255, 255, 255), #color=(245,66,230),\n","                              thickness=2, circle_radius=2)\n","                             ) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GkZI2_fnn0Bn","outputId":"c2748d30-a2b5-48d9-9fd0-9b93a04e4ad2"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n","<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"]}],"source":["cap = cv2.VideoCapture(0)\n","# Set mediapipe model \n","with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","    while cap.isOpened():\n","\n","        # Read feed\n","        ret, frame = cap.read()\n","\n","        # Make detections\n","        image, results = mediapipe_detection(frame, holistic)\n","        print(results)\n","        \n","        #converting image to black\n","        image = np.zeros(frame.shape, np.uint8)\n","        # Draw landmarks\n","        draw_styled_landmarks(image, results)\n","\n","        # Show to screen\n","        cv2.imshow('OpenCV Feed', image)\n","\n","        # Break gracefully\n","        if cv2.waitKey(10) & 0xFF == ord('q'):\n","            break\n","    cap.release()\n","    cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"nfWlWDGln0Bu"},"source":["# Setup folder for collection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IE5isiz7n0By"},"outputs":[],"source":["# Path for exported data, numpy arrays\n","DATA_PATH = os.path.join('MP_Data') \n","\n","# Actions that we try to detect\n","# actions = np.array(['hello', 'thanks', 'fine'])\n","actions = np.array(['help', 'thirsty'])\n","\n","# Thirty videos worth of data\n","no_sequences = 15\n","\n","# Videos are going to be 30 frames in length\n","sequence_length = 15\n","\n","# Folder start\n","start_folder = 15"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tAPQFmVEn0B2"},"outputs":[],"source":["for action in actions: \n","    # dirmax = np.max(np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int))\n","    for sequence in range(1,no_sequences+1):\n","        try: \n","            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n","        except:\n","            pass"]},{"cell_type":"markdown","metadata":{"id":"9COauOQmn0B4"},"source":["# Creating Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGtBBiBun0B6","outputId":"f501486f-0fc3-499d-ff6a-bb9631532eae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting Collection\n","Collecting frames for hello video number 1\n","Collecting frames for hello Video Number 1\n","Collecting frames for hello Video Number 1\n","Collecting frames for hello Video Number 1\n","Collecting frames for hello Video Number 1\n","Collecting frames for hello Video Number 1\n","Collecting frames for hello Video Number 1\n","Collecting frames for hello Video Number 1\n","Collecting frames for hello Video Number 1\n","Collecting frames for hello Video Number 1\n","Collecting frames for hello Video Number 1\n","Collecting frames for hello Video Number 1\n","Collecting frames for hello Video Number 1\n","Collecting frames for hello Video Number 1\n","Collecting frames for hello Video Number 1\n","Starting Collection\n","Collecting frames for hello video number 2\n","Collecting frames for hello Video Number 2\n","Collecting frames for hello Video Number 2\n","Collecting frames for hello Video Number 2\n","Collecting frames for hello Video Number 2\n","Collecting frames for hello Video Number 2\n","Collecting frames for hello Video Number 2\n","Collecting frames for hello Video Number 2\n","Collecting frames for hello Video Number 2\n","Collecting frames for hello Video Number 2\n","Collecting frames for hello Video Number 2\n","Collecting frames for hello Video Number 2\n","Collecting frames for hello Video Number 2\n","Collecting frames for hello Video Number 2\n","Collecting frames for hello Video Number 2\n","Starting Collection\n","Collecting frames for hello video number 3\n","Collecting frames for hello Video Number 3\n","Collecting frames for hello Video Number 3\n","Collecting frames for hello Video Number 3\n","Collecting frames for hello Video Number 3\n","Collecting frames for hello Video Number 3\n","Collecting frames for hello Video Number 3\n","Collecting frames for hello Video Number 3\n","Collecting frames for hello Video Number 3\n","Collecting frames for hello Video Number 3\n","Collecting frames for hello Video Number 3\n","Collecting frames for hello Video Number 3\n","Collecting frames for hello Video Number 3\n","Collecting frames for hello Video Number 3\n","Collecting frames for hello Video Number 3\n","Starting Collection\n","Collecting frames for hello video number 4\n","Collecting frames for hello Video Number 4\n","Collecting frames for hello Video Number 4\n","Collecting frames for hello Video Number 4\n","Collecting frames for hello Video Number 4\n","Collecting frames for hello Video Number 4\n","Collecting frames for hello Video Number 4\n","Collecting frames for hello Video Number 4\n","Collecting frames for hello Video Number 4\n","Collecting frames for hello Video Number 4\n","Collecting frames for hello Video Number 4\n","Collecting frames for hello Video Number 4\n","Collecting frames for hello Video Number 4\n","Collecting frames for hello Video Number 4\n","Collecting frames for hello Video Number 4\n","Starting Collection\n","Collecting frames for hello video number 5\n","Collecting frames for hello Video Number 5\n","Collecting frames for hello Video Number 5\n","Collecting frames for hello Video Number 5\n","Collecting frames for hello Video Number 5\n"]}],"source":["cap = cv2.VideoCapture(0)\n","# Set mediapipe model \n","with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","    \n","    # NEW LOOP\n","    # Loop through actions\n","    b=False\n","    for action in actions:\n","        # Loop through sequences aka videos\n","        for sequence in range(1, no_sequences+1):\n","            # Loop through video length aka sequence length\n","            for frame_num in range(sequence_length):\n","\n","                # Read feed\n","                ret, frame = cap.read()\n","\n","                # cv2.imshow(\"frame\",frame)\n","                # Make detections\n","                image, results = mediapipe_detection(frame, holistic)\n","                image = np.zeros(frame.shape, np.uint8)\n","                # Draw landmarks\n","                draw_styled_landmarks(image, results)\n","                \n","                # NEW Apply wait logic\n","                if frame_num == 0: \n","                    # cv2.putText(image, 'STARTING COLLECTION', (120,200), \n","                    #            cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n","                    # cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n","                    #            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n","                    print(\"Starting Collection\")\n","                    print(\"Collecting frames for {} video number {}\".format(action, str(sequence)))\n","                    # Show to screen\n","                    cv2.imshow('OpenCV Feed', image)\n","                    cv2.waitKey(2000)\n","                else: \n","                    # cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n","                            #    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n","                    print('Collecting frames for {} Video Number {}'.format(action, sequence))\n","                    # Show to screen\n","                    cv2.imshow('OpenCV Feed', image)\n","                \n","                # NEW Export keypoints\n","                # keypoints = extract_keypoints(results)\n","                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n","                # np.save(npy_path, keypoints)\n","                # image_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num), \"i\", \".png\")\n","                # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","                cv2.imwrite(npy_path + \".png\", image)\n","\n","                # Break gracefully\n","                if cv2.waitKey(10) & 0xFF == ord('q'):\n","                    b=True\n","                    break\n","            if b==True:\n","                break\n","        if b==True:\n","            break\n","    cv2.destroyAllWindows()\n","    cap.release()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Oap_ZDwn0B8"},"outputs":[],"source":[""]}],"metadata":{"interpreter":{"hash":"0aca11b4d3f3d4b68700af3dfa143e9c139cb5d21c28aebb64ef5022db915092"},"kernelspec":{"display_name":"Python 3.9.9 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"orig_nbformat":4,"colab":{"name":"CreateDataset.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}